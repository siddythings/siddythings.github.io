<!doctype html><html lang=en dir=auto><head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>5 Essential Chunking Strategies for RAG Applications | siddythings</title>
<meta name=keywords content="ai,llm,rag,nlp,embeddings,chunking"><meta name=description content="Explore five powerful chunking strategies for Retrieval-Augmented Generation (RAG) applications, from fixed-size to LLM-based approaches. Learn how each strategy impacts the quality of your RAG system and when to use them."><meta name=author content="sid"><link rel=canonical href=http://localhost:1313/blogs/rag-chunking-strategies/><meta name=google-site-verification content="XYZabc"><meta name=yandex-verification content="XYZabc"><meta name=msvalidate.01 content="XYZabc"><link crossorigin=anonymous href=/assets/css/stylesheet.0c4fd3725171366f335155acde6fb3c7b7042cd2fd075bebf5023d9b28a701b2.css integrity="sha256-DE/TclFxNm8zUVWs3m+zx7cELNL9B1vr9QI9myinAbI=" rel="preload stylesheet" as=style><link rel=icon href=http://localhost:1313/favicon.ico><link rel=icon type=image/png sizes=16x16 href=http://localhost:1313/favicon-16x16.png><link rel=icon type=image/png sizes=32x32 href=http://localhost:1313/favicon-32x32.png><link rel=apple-touch-icon href=http://localhost:1313/apple-touch-icon.png><link rel=mask-icon href=http://localhost:1313/safari-pinned-tab.svg><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=http://localhost:1313/blogs/rag-chunking-strategies/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--hljs-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><script>var doNotTrack=!1;doNotTrack||(function(e,t,n,s,o,i,a){e.GoogleAnalyticsObject=o,e[o]=e[o]||function(){(e[o].q=e[o].q||[]).push(arguments)},e[o].l=1*new Date,i=t.createElement(n),a=t.getElementsByTagName(n)[0],i.async=1,i.src=s,a.parentNode.insertBefore(i,a)}(window,document,"script","https://www.google-analytics.com/analytics.js","ga"),ga("create","UA-123-45","auto"),ga("send","pageview"))</script><meta property="og:title" content="5 Essential Chunking Strategies for RAG Applications"><meta property="og:description" content="Explore five powerful chunking strategies for Retrieval-Augmented Generation (RAG) applications, from fixed-size to LLM-based approaches. Learn how each strategy impacts the quality of your RAG system and when to use them."><meta property="og:type" content="article"><meta property="og:url" content="http://localhost:1313/blogs/rag-chunking-strategies/"><meta property="og:image" content="http://localhost:1313/images/rag-chunking-title.png"><meta property="article:section" content="blogs"><meta property="og:site_name" content="siddythings"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="http://localhost:1313/images/rag-chunking-title.png"><meta name=twitter:title content="5 Essential Chunking Strategies for RAG Applications"><meta name=twitter:description content="Explore five powerful chunking strategies for Retrieval-Augmented Generation (RAG) applications, from fixed-size to LLM-based approaches. Learn how each strategy impacts the quality of your RAG system and when to use them."><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Blogs","item":"http://localhost:1313/blogs/"},{"@type":"ListItem","position":2,"name":"5 Essential Chunking Strategies for RAG Applications","item":"http://localhost:1313/blogs/rag-chunking-strategies/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"5 Essential Chunking Strategies for RAG Applications","name":"5 Essential Chunking Strategies for RAG Applications","description":"Explore five powerful chunking strategies for Retrieval-Augmented Generation (RAG) applications, from fixed-size to LLM-based approaches. Learn how each strategy impacts the quality of your RAG system and when to use them.","keywords":["ai","llm","rag","nlp","embeddings","chunking"],"articleBody":"Introduction Retrieval-Augmented Generation (RAG) has revolutionized how we build AI applications by combining the power of large language models with external knowledge bases. At the heart of every RAG system lies a crucial step: chunking. This process of dividing large documents into manageable pieces is essential for efficient retrieval and high-quality responses.\nUnderstanding RAG and Chunking Before diving into chunking strategies, letâ€™s understand the typical RAG workflow:\nDocument Processing: Large documents are split into smaller chunks Vector Storage: These chunks are converted into vector embeddings Query Matching: Incoming queries are matched against stored vectors Response Generation: The most relevant chunks are fed to the LLM along with the query The chunking step is critical because it:\nEnsures text fits within embedding model input limits Enhances retrieval efficiency and accuracy Directly impacts the quality of generated responses Five Essential Chunking Strategies 1. Fixed-Size Chunking The most straightforward approach, fixed-size chunking splits text into uniform segments based on:\nCharacter count Word count Token count def fixed_size_chunking(text, chunk_size=1000, overlap=100): chunks = [] start = 0 text_length = len(text) while start \u003c text_length: end = start + chunk_size chunk = text[start:end] chunks.append(chunk) start = end - overlap return chunks Advantages:\nSimple to implement Facilitates batch processing Consistent chunk sizes Limitations:\nMay break sentences mid-way Can split important information across chunks Lacks semantic awareness 2. Semantic Chunking This strategy creates chunks based on semantic similarity between text segments:\nfrom sentence_transformers import SentenceTransformer import numpy as np def semantic_chunking(text, similarity_threshold=0.8): model = SentenceTransformer('all-MiniLM-L6-v2') sentences = text.split('. ') chunks = [] current_chunk = [] for i in range(len(sentences)): if not current_chunk: current_chunk.append(sentences[i]) continue # Calculate similarity between current sentence and chunk current_embedding = model.encode(sentences[i]) chunk_embedding = model.encode(' '.join(current_chunk)) similarity = np.dot(current_embedding, chunk_embedding) / ( np.linalg.norm(current_embedding) * np.linalg.norm(chunk_embedding) ) if similarity \u003e= similarity_threshold: current_chunk.append(sentences[i]) else: chunks.append(' '.join(current_chunk)) current_chunk = [sentences[i]] if current_chunk: chunks.append(' '.join(current_chunk)) return chunks Advantages:\nMaintains natural language flow Preserves complete ideas Improves retrieval accuracy Limitations:\nRequires similarity threshold tuning More computationally intensive May vary in effectiveness across documents 3. Recursive Chunking A hierarchical approach that splits text based on natural separators and size limits:\ndef recursive_chunking(text, max_chunk_size=1000): # First split by paragraphs paragraphs = text.split('\\n\\n') chunks = [] for paragraph in paragraphs: if len(paragraph) \u003c= max_chunk_size: chunks.append(paragraph) else: # Recursively split large paragraphs sub_chunks = recursive_chunking(paragraph, max_chunk_size) chunks.extend(sub_chunks) return chunks Advantages:\nPreserves document structure Maintains semantic coherence Flexible chunk sizes Limitations:\nMore complex implementation Higher computational overhead May require multiple passes 4. Document Structure-Based Chunking Leverages document formatting to create meaningful chunks:\nfrom bs4 import BeautifulSoup def structure_based_chunking(html_content): soup = BeautifulSoup(html_content, 'html.parser') chunks = [] # Split by headings for heading in soup.find_all(['h1', 'h2', 'h3']): section = [] current = heading.next_sibling while current and current.name not in ['h1', 'h2', 'h3']: if current.string: section.append(current.string) current = current.next_sibling if section: chunks.append(' '.join(section)) return chunks Advantages:\nMaintains document structure Aligns with logical sections Preserves context Limitations:\nRequires structured documents May produce uneven chunk sizes Less effective with unstructured text 5. LLM-Based Chunking Uses language models to create semantically meaningful chunks:\nfrom langchain.text_splitter import LLMTextSplitter from langchain.llms import OpenAI def llm_based_chunking(text, chunk_size=1000): llm = OpenAI(temperature=0) text_splitter = LLMTextSplitter( llm=llm, chunk_size=chunk_size, chunk_overlap=100 ) chunks = text_splitter.split_text(text) return chunks Advantages:\nHigh semantic accuracy Context-aware splitting Intelligent chunk boundaries Limitations:\nMost computationally expensive Limited by LLM context window Higher API costs Choosing the Right Strategy The choice of chunking strategy depends on several factors:\nContent Nature\nStructured vs. unstructured text Document length and complexity Language and domain specificity Technical Constraints\nAvailable computational resources Embedding model limitations Processing time requirements Quality Requirements\nDesired response accuracy Context preservation needs Retrieval efficiency goals Best Practices Experiment and Evaluate\nTest different strategies with your specific content Measure impact on retrieval quality Monitor response coherence Hybrid Approaches\nCombine strategies for better results Use different strategies for different content types Implement fallback mechanisms Optimization\nFine-tune chunk sizes Adjust overlap parameters Monitor performance metrics ","wordCount":"645","inLanguage":"en","image":"http://localhost:1313/images/rag-chunking-title.png","datePublished":"0001-01-01T00:00:00Z","dateModified":"0001-01-01T00:00:00Z","author":{"@type":"Person","name":"sid"},"mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:1313/blogs/rag-chunking-strategies/"},"publisher":{"@type":"Organization","name":"siddythings","logo":{"@type":"ImageObject","url":"http://localhost:1313/favicon.ico"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=http://localhost:1313/ accesskey=h title="siddythings (Alt + H)"><img src=http://localhost:1313/avatartion_huab6c5fec9f4bfadb16d15df5a7fa7d1e_46872_0x35_resize_box_3.png alt aria-label=logo height=35>siddythings</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" xmlns="http://www.w3.org/2000/svg" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button><ul class=lang-switch><li>|</li></ul></div></div><ul id=menu><li><a href=http://localhost:1313/blogs/ title=blogs><span>blogs</span></a></li><li><a href=http://localhost:1313/ai-agents/ title=ai-agents><span>ai-agents</span></a></li><li><a href=http://localhost:1313/projects/ title=projects><span>projects</span></a></li><li><a href=http://localhost:1313/about/ title=about><span>about</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=http://localhost:1313/>Home</a>&nbsp;Â»&nbsp;<a href=http://localhost:1313/blogs/>Blogs</a></div><h1 class=post-title>5 Essential Chunking Strategies for RAG Applications</h1><div class=post-meta>4 min&nbsp;Â·&nbsp;645 words&nbsp;Â·&nbsp;sid</div></header><figure class=entry-cover><img loading=lazy src=http://localhost:1313/images/rag-chunking-title.png alt></figure><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><nav id=TableOfContents><ul><li><a href=#introduction>Introduction</a></li><li><a href=#understanding-rag-and-chunking>Understanding RAG and Chunking</a></li><li><a href=#five-essential-chunking-strategies>Five Essential Chunking Strategies</a><ul><li><a href=#1-fixed-size-chunking>1. Fixed-Size Chunking</a></li><li><a href=#2-semantic-chunking>2. Semantic Chunking</a></li><li><a href=#3-recursive-chunking>3. Recursive Chunking</a></li><li><a href=#4-document-structure-based-chunking>4. Document Structure-Based Chunking</a></li><li><a href=#5-llm-based-chunking>5. LLM-Based Chunking</a></li></ul></li><li><a href=#choosing-the-right-strategy>Choosing the Right Strategy</a></li><li><a href=#best-practices>Best Practices</a></li></ul></nav></div></details></div><div class=post-content><h2 id=introduction>Introduction<a hidden class=anchor aria-hidden=true href=#introduction>#</a></h2><p>Retrieval-Augmented Generation (RAG) has revolutionized how we build AI applications by combining the power of large language models with external knowledge bases. At the heart of every RAG system lies a crucial step: chunking. This process of dividing large documents into manageable pieces is essential for efficient retrieval and high-quality responses.</p><h2 id=understanding-rag-and-chunking>Understanding RAG and Chunking<a hidden class=anchor aria-hidden=true href=#understanding-rag-and-chunking>#</a></h2><p>Before diving into chunking strategies, let&rsquo;s understand the typical RAG workflow:</p><ol><li><strong>Document Processing</strong>: Large documents are split into smaller chunks</li><li><strong>Vector Storage</strong>: These chunks are converted into vector embeddings</li><li><strong>Query Matching</strong>: Incoming queries are matched against stored vectors</li><li><strong>Response Generation</strong>: The most relevant chunks are fed to the LLM along with the query</li></ol><p>The chunking step is critical because it:</p><ul><li>Ensures text fits within embedding model input limits</li><li>Enhances retrieval efficiency and accuracy</li><li>Directly impacts the quality of generated responses</li></ul><h2 id=five-essential-chunking-strategies>Five Essential Chunking Strategies<a hidden class=anchor aria-hidden=true href=#five-essential-chunking-strategies>#</a></h2><h3 id=1-fixed-size-chunking>1. Fixed-Size Chunking<a hidden class=anchor aria-hidden=true href=#1-fixed-size-chunking>#</a></h3><p>The most straightforward approach, fixed-size chunking splits text into uniform segments based on:</p><ul><li>Character count</li><li>Word count</li><li>Token count</li></ul><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>def</span> <span class=nf>fixed_size_chunking</span><span class=p>(</span><span class=n>text</span><span class=p>,</span> <span class=n>chunk_size</span><span class=o>=</span><span class=mi>1000</span><span class=p>,</span> <span class=n>overlap</span><span class=o>=</span><span class=mi>100</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=n>chunks</span> <span class=o>=</span> <span class=p>[]</span>
</span></span><span class=line><span class=cl>    <span class=n>start</span> <span class=o>=</span> <span class=mi>0</span>
</span></span><span class=line><span class=cl>    <span class=n>text_length</span> <span class=o>=</span> <span class=nb>len</span><span class=p>(</span><span class=n>text</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    
</span></span><span class=line><span class=cl>    <span class=k>while</span> <span class=n>start</span> <span class=o>&lt;</span> <span class=n>text_length</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=n>end</span> <span class=o>=</span> <span class=n>start</span> <span class=o>+</span> <span class=n>chunk_size</span>
</span></span><span class=line><span class=cl>        <span class=n>chunk</span> <span class=o>=</span> <span class=n>text</span><span class=p>[</span><span class=n>start</span><span class=p>:</span><span class=n>end</span><span class=p>]</span>
</span></span><span class=line><span class=cl>        <span class=n>chunks</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=n>chunk</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=n>start</span> <span class=o>=</span> <span class=n>end</span> <span class=o>-</span> <span class=n>overlap</span>
</span></span><span class=line><span class=cl>    
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>chunks</span>
</span></span></code></pre></div><p><strong>Advantages:</strong></p><ul><li>Simple to implement</li><li>Facilitates batch processing</li><li>Consistent chunk sizes</li></ul><p><strong>Limitations:</strong></p><ul><li>May break sentences mid-way</li><li>Can split important information across chunks</li><li>Lacks semantic awareness</li></ul><h3 id=2-semantic-chunking>2. Semantic Chunking<a hidden class=anchor aria-hidden=true href=#2-semantic-chunking>#</a></h3><p>This strategy creates chunks based on semantic similarity between text segments:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>sentence_transformers</span> <span class=kn>import</span> <span class=n>SentenceTransformer</span>
</span></span><span class=line><span class=cl><span class=kn>import</span> <span class=nn>numpy</span> <span class=k>as</span> <span class=nn>np</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>semantic_chunking</span><span class=p>(</span><span class=n>text</span><span class=p>,</span> <span class=n>similarity_threshold</span><span class=o>=</span><span class=mf>0.8</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=n>model</span> <span class=o>=</span> <span class=n>SentenceTransformer</span><span class=p>(</span><span class=s1>&#39;all-MiniLM-L6-v2&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>sentences</span> <span class=o>=</span> <span class=n>text</span><span class=o>.</span><span class=n>split</span><span class=p>(</span><span class=s1>&#39;. &#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>chunks</span> <span class=o>=</span> <span class=p>[]</span>
</span></span><span class=line><span class=cl>    <span class=n>current_chunk</span> <span class=o>=</span> <span class=p>[]</span>
</span></span><span class=line><span class=cl>    
</span></span><span class=line><span class=cl>    <span class=k>for</span> <span class=n>i</span> <span class=ow>in</span> <span class=nb>range</span><span class=p>(</span><span class=nb>len</span><span class=p>(</span><span class=n>sentences</span><span class=p>)):</span>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=ow>not</span> <span class=n>current_chunk</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=n>current_chunk</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=n>sentences</span><span class=p>[</span><span class=n>i</span><span class=p>])</span>
</span></span><span class=line><span class=cl>            <span class=k>continue</span>
</span></span><span class=line><span class=cl>            
</span></span><span class=line><span class=cl>        <span class=c1># Calculate similarity between current sentence and chunk</span>
</span></span><span class=line><span class=cl>        <span class=n>current_embedding</span> <span class=o>=</span> <span class=n>model</span><span class=o>.</span><span class=n>encode</span><span class=p>(</span><span class=n>sentences</span><span class=p>[</span><span class=n>i</span><span class=p>])</span>
</span></span><span class=line><span class=cl>        <span class=n>chunk_embedding</span> <span class=o>=</span> <span class=n>model</span><span class=o>.</span><span class=n>encode</span><span class=p>(</span><span class=s1>&#39; &#39;</span><span class=o>.</span><span class=n>join</span><span class=p>(</span><span class=n>current_chunk</span><span class=p>))</span>
</span></span><span class=line><span class=cl>        <span class=n>similarity</span> <span class=o>=</span> <span class=n>np</span><span class=o>.</span><span class=n>dot</span><span class=p>(</span><span class=n>current_embedding</span><span class=p>,</span> <span class=n>chunk_embedding</span><span class=p>)</span> <span class=o>/</span> <span class=p>(</span>
</span></span><span class=line><span class=cl>            <span class=n>np</span><span class=o>.</span><span class=n>linalg</span><span class=o>.</span><span class=n>norm</span><span class=p>(</span><span class=n>current_embedding</span><span class=p>)</span> <span class=o>*</span> <span class=n>np</span><span class=o>.</span><span class=n>linalg</span><span class=o>.</span><span class=n>norm</span><span class=p>(</span><span class=n>chunk_embedding</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=p>)</span>
</span></span><span class=line><span class=cl>        
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=n>similarity</span> <span class=o>&gt;=</span> <span class=n>similarity_threshold</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=n>current_chunk</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=n>sentences</span><span class=p>[</span><span class=n>i</span><span class=p>])</span>
</span></span><span class=line><span class=cl>        <span class=k>else</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=n>chunks</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=s1>&#39; &#39;</span><span class=o>.</span><span class=n>join</span><span class=p>(</span><span class=n>current_chunk</span><span class=p>))</span>
</span></span><span class=line><span class=cl>            <span class=n>current_chunk</span> <span class=o>=</span> <span class=p>[</span><span class=n>sentences</span><span class=p>[</span><span class=n>i</span><span class=p>]]</span>
</span></span><span class=line><span class=cl>    
</span></span><span class=line><span class=cl>    <span class=k>if</span> <span class=n>current_chunk</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=n>chunks</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=s1>&#39; &#39;</span><span class=o>.</span><span class=n>join</span><span class=p>(</span><span class=n>current_chunk</span><span class=p>))</span>
</span></span><span class=line><span class=cl>    
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>chunks</span>
</span></span></code></pre></div><p><strong>Advantages:</strong></p><ul><li>Maintains natural language flow</li><li>Preserves complete ideas</li><li>Improves retrieval accuracy</li></ul><p><strong>Limitations:</strong></p><ul><li>Requires similarity threshold tuning</li><li>More computationally intensive</li><li>May vary in effectiveness across documents</li></ul><h3 id=3-recursive-chunking>3. Recursive Chunking<a hidden class=anchor aria-hidden=true href=#3-recursive-chunking>#</a></h3><p>A hierarchical approach that splits text based on natural separators and size limits:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>def</span> <span class=nf>recursive_chunking</span><span class=p>(</span><span class=n>text</span><span class=p>,</span> <span class=n>max_chunk_size</span><span class=o>=</span><span class=mi>1000</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=c1># First split by paragraphs</span>
</span></span><span class=line><span class=cl>    <span class=n>paragraphs</span> <span class=o>=</span> <span class=n>text</span><span class=o>.</span><span class=n>split</span><span class=p>(</span><span class=s1>&#39;</span><span class=se>\n\n</span><span class=s1>&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>chunks</span> <span class=o>=</span> <span class=p>[]</span>
</span></span><span class=line><span class=cl>    
</span></span><span class=line><span class=cl>    <span class=k>for</span> <span class=n>paragraph</span> <span class=ow>in</span> <span class=n>paragraphs</span><span class=p>:</span>
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=nb>len</span><span class=p>(</span><span class=n>paragraph</span><span class=p>)</span> <span class=o>&lt;=</span> <span class=n>max_chunk_size</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=n>chunks</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=n>paragraph</span><span class=p>)</span>
</span></span><span class=line><span class=cl>        <span class=k>else</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=c1># Recursively split large paragraphs</span>
</span></span><span class=line><span class=cl>            <span class=n>sub_chunks</span> <span class=o>=</span> <span class=n>recursive_chunking</span><span class=p>(</span><span class=n>paragraph</span><span class=p>,</span> <span class=n>max_chunk_size</span><span class=p>)</span>
</span></span><span class=line><span class=cl>            <span class=n>chunks</span><span class=o>.</span><span class=n>extend</span><span class=p>(</span><span class=n>sub_chunks</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>chunks</span>
</span></span></code></pre></div><p><strong>Advantages:</strong></p><ul><li>Preserves document structure</li><li>Maintains semantic coherence</li><li>Flexible chunk sizes</li></ul><p><strong>Limitations:</strong></p><ul><li>More complex implementation</li><li>Higher computational overhead</li><li>May require multiple passes</li></ul><h3 id=4-document-structure-based-chunking>4. Document Structure-Based Chunking<a hidden class=anchor aria-hidden=true href=#4-document-structure-based-chunking>#</a></h3><p>Leverages document formatting to create meaningful chunks:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>bs4</span> <span class=kn>import</span> <span class=n>BeautifulSoup</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>structure_based_chunking</span><span class=p>(</span><span class=n>html_content</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=n>soup</span> <span class=o>=</span> <span class=n>BeautifulSoup</span><span class=p>(</span><span class=n>html_content</span><span class=p>,</span> <span class=s1>&#39;html.parser&#39;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>chunks</span> <span class=o>=</span> <span class=p>[]</span>
</span></span><span class=line><span class=cl>    
</span></span><span class=line><span class=cl>    <span class=c1># Split by headings</span>
</span></span><span class=line><span class=cl>    <span class=k>for</span> <span class=n>heading</span> <span class=ow>in</span> <span class=n>soup</span><span class=o>.</span><span class=n>find_all</span><span class=p>([</span><span class=s1>&#39;h1&#39;</span><span class=p>,</span> <span class=s1>&#39;h2&#39;</span><span class=p>,</span> <span class=s1>&#39;h3&#39;</span><span class=p>]):</span>
</span></span><span class=line><span class=cl>        <span class=n>section</span> <span class=o>=</span> <span class=p>[]</span>
</span></span><span class=line><span class=cl>        <span class=n>current</span> <span class=o>=</span> <span class=n>heading</span><span class=o>.</span><span class=n>next_sibling</span>
</span></span><span class=line><span class=cl>        
</span></span><span class=line><span class=cl>        <span class=k>while</span> <span class=n>current</span> <span class=ow>and</span> <span class=n>current</span><span class=o>.</span><span class=n>name</span> <span class=ow>not</span> <span class=ow>in</span> <span class=p>[</span><span class=s1>&#39;h1&#39;</span><span class=p>,</span> <span class=s1>&#39;h2&#39;</span><span class=p>,</span> <span class=s1>&#39;h3&#39;</span><span class=p>]:</span>
</span></span><span class=line><span class=cl>            <span class=k>if</span> <span class=n>current</span><span class=o>.</span><span class=n>string</span><span class=p>:</span>
</span></span><span class=line><span class=cl>                <span class=n>section</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=n>current</span><span class=o>.</span><span class=n>string</span><span class=p>)</span>
</span></span><span class=line><span class=cl>            <span class=n>current</span> <span class=o>=</span> <span class=n>current</span><span class=o>.</span><span class=n>next_sibling</span>
</span></span><span class=line><span class=cl>        
</span></span><span class=line><span class=cl>        <span class=k>if</span> <span class=n>section</span><span class=p>:</span>
</span></span><span class=line><span class=cl>            <span class=n>chunks</span><span class=o>.</span><span class=n>append</span><span class=p>(</span><span class=s1>&#39; &#39;</span><span class=o>.</span><span class=n>join</span><span class=p>(</span><span class=n>section</span><span class=p>))</span>
</span></span><span class=line><span class=cl>    
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>chunks</span>
</span></span></code></pre></div><p><strong>Advantages:</strong></p><ul><li>Maintains document structure</li><li>Aligns with logical sections</li><li>Preserves context</li></ul><p><strong>Limitations:</strong></p><ul><li>Requires structured documents</li><li>May produce uneven chunk sizes</li><li>Less effective with unstructured text</li></ul><h3 id=5-llm-based-chunking>5. LLM-Based Chunking<a hidden class=anchor aria-hidden=true href=#5-llm-based-chunking>#</a></h3><p>Uses language models to create semantically meaningful chunks:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>langchain.text_splitter</span> <span class=kn>import</span> <span class=n>LLMTextSplitter</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>langchain.llms</span> <span class=kn>import</span> <span class=n>OpenAI</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>def</span> <span class=nf>llm_based_chunking</span><span class=p>(</span><span class=n>text</span><span class=p>,</span> <span class=n>chunk_size</span><span class=o>=</span><span class=mi>1000</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=n>llm</span> <span class=o>=</span> <span class=n>OpenAI</span><span class=p>(</span><span class=n>temperature</span><span class=o>=</span><span class=mi>0</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=n>text_splitter</span> <span class=o>=</span> <span class=n>LLMTextSplitter</span><span class=p>(</span>
</span></span><span class=line><span class=cl>        <span class=n>llm</span><span class=o>=</span><span class=n>llm</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>chunk_size</span><span class=o>=</span><span class=n>chunk_size</span><span class=p>,</span>
</span></span><span class=line><span class=cl>        <span class=n>chunk_overlap</span><span class=o>=</span><span class=mi>100</span>
</span></span><span class=line><span class=cl>    <span class=p>)</span>
</span></span><span class=line><span class=cl>    
</span></span><span class=line><span class=cl>    <span class=n>chunks</span> <span class=o>=</span> <span class=n>text_splitter</span><span class=o>.</span><span class=n>split_text</span><span class=p>(</span><span class=n>text</span><span class=p>)</span>
</span></span><span class=line><span class=cl>    <span class=k>return</span> <span class=n>chunks</span>
</span></span></code></pre></div><p><strong>Advantages:</strong></p><ul><li>High semantic accuracy</li><li>Context-aware splitting</li><li>Intelligent chunk boundaries</li></ul><p><strong>Limitations:</strong></p><ul><li>Most computationally expensive</li><li>Limited by LLM context window</li><li>Higher API costs</li></ul><h2 id=choosing-the-right-strategy>Choosing the Right Strategy<a hidden class=anchor aria-hidden=true href=#choosing-the-right-strategy>#</a></h2><p>The choice of chunking strategy depends on several factors:</p><ol><li><p><strong>Content Nature</strong></p><ul><li>Structured vs. unstructured text</li><li>Document length and complexity</li><li>Language and domain specificity</li></ul></li><li><p><strong>Technical Constraints</strong></p><ul><li>Available computational resources</li><li>Embedding model limitations</li><li>Processing time requirements</li></ul></li><li><p><strong>Quality Requirements</strong></p><ul><li>Desired response accuracy</li><li>Context preservation needs</li><li>Retrieval efficiency goals</li></ul></li></ol><h2 id=best-practices>Best Practices<a hidden class=anchor aria-hidden=true href=#best-practices>#</a></h2><ol><li><p><strong>Experiment and Evaluate</strong></p><ul><li>Test different strategies with your specific content</li><li>Measure impact on retrieval quality</li><li>Monitor response coherence</li></ul></li><li><p><strong>Hybrid Approaches</strong></p><ul><li>Combine strategies for better results</li><li>Use different strategies for different content types</li><li>Implement fallback mechanisms</li></ul></li><li><p><strong>Optimization</strong></p><ul><li>Fine-tune chunk sizes</li><li>Adjust overlap parameters</li><li>Monitor performance metrics</li></ul></li></ol></div><footer class=post-footer><ul class=post-tags><li><a href=http://localhost:1313/tags/ai/>Ai</a></li><li><a href=http://localhost:1313/tags/llm/>Llm</a></li><li><a href=http://localhost:1313/tags/rag/>Rag</a></li><li><a href=http://localhost:1313/tags/nlp/>Nlp</a></li><li><a href=http://localhost:1313/tags/embeddings/>Embeddings</a></li><li><a href=http://localhost:1313/tags/chunking/>Chunking</a></li></ul><nav class=paginav><a class=prev href=http://localhost:1313/blogs/agent2agent-protocol/><span class=title>Â« Prev</span><br><span>Building Collaborative AI Systems: A Deep Dive into Agent2Agent Protocol</span>
</a><a class=next href=http://localhost:1313/blogs/building-local-mcp-client/><span class=title>Next Â»</span><br><span>Building a 100% Local MCP Client</span></a></nav></footer></article></main><footer class=footer><span>&copy; 2025 <a href=http://localhost:1313/>siddythings</a></span>
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>